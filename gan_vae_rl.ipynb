{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)\n",
    "Generative Adversarial Networks, or GANs, are a fascinating idea in deep learning. The goal of GANs is to generate new, synthetic data that resembles some existing real data.\n",
    "\n",
    "### What is a Generative Adversarial Network (GAN)?\n",
    "GANs consist of two parts: a generator and a discriminator. The generator creates the data, and the discriminator evaluates the data. The fascinating thing is that these two parts are in a game (hence the name adversarial). The generator tries to generate data that looks real, while the discriminator tries to tell apart the real data from the fake. They both get better over time until, hopefully, the generator generates real-looking data and the discriminator can't tell if it's real or fake.\n",
    "\n",
    "Let's use a simple analogy to understand this better. Think of a police investigator (the discriminator) and a counterfeiter (the generator). The counterfeiter wants to create counterfeit money that the investigator can't distinguish from real money. In the beginning, the counterfeiter might not be good, and the investigator catches him easily. But as time goes by, the counterfeiter learns from his mistakes and becomes better, and so does the investigator. In the end, the counterfeiter becomes so good at his job that the investigator can't distinguish the counterfeit money from the real ones.\n",
    "\n",
    "### Basic GAN Implementation\n",
    "Let's look at a very simplified PyTorch code of how a GAN could be implemented. Note that this is a simplified version and actual implementation could vary based on the type of GAN and the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Here we are defining our generator network\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Here we are defining our discriminator network\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Creating instances of generator and discriminator\n",
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Establish convention for real and fake labels\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.0002)\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        real = data[0]\n",
    "        batch_size = real.size(0)\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float)\n",
    "        output = netD(real).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        noise = torch.randn(batch_size, 100)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two neural networks, the Generator (G) and the Discriminator (D), each with its own optimizer. They play a two-player min-max game with the value function V(G,D):\n",
    "\n",
    "minG maxD V(D,G) = E[log(D(x))] + E[log(1 - D(G(z)))].\n",
    "\n",
    "In simple terms, D tries to maximize its probability of assigning the correct label to both training examples and samples from G, and G tries to minimize the probability that D will predict its samples as being fake."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders (VAEs)\n",
    "Autoencoders are a type of neural network architecture used for learning efficient codings of input data. They are \"self-supervised,\" which means they learn from the input data itself, without the need for labels. They consist of an encoder, which compresses the input data, and a decoder, which reconstructs the original data from the compressed version.\n",
    "\n",
    "Variational Autoencoders are a special type of autoencoder with added constraints on the encoded representations being learned. More specifically, they are a type of probabilistic approach to encoding where the data is transformed into a normal distribution. When decoding, samples from this distribution are transformed back into the input data. This approach helps generate more robust models and also helps generate new data.\n",
    "\n",
    "### Basic VAE Implementation\n",
    "Below is a simplified version of a VAE implemented in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AB012DH\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\AB012DH\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\AB012DH\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20) # mu layer\n",
    "        self.fc22 = nn.Linear(400, 20) # logvariance layer\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This VAE is designed to work on MNIST-like data (28x28 grayscale images, thus 784 pixels in total), and it's composed of two main parts: the encoder and the decoder.\n",
    "\n",
    "The encoder consists of two fully-connected layers that take the input x and output two parameters: mu (mean) and logvar (logarithm of the variance). These parameters represent the learned normal distribution.\n",
    "\n",
    "The reparameterization step is a trick that allows us to backpropagate gradients through the random sampling operation. It generates a latent vector z by taking a random sample from the normal distribution defined by mu and logvar.\n",
    "\n",
    "The decoder then takes the latent vector z and reconstructs the input data.\n",
    "\n",
    "VAEs introduce a probabilistic spin into the world of autoencoders, opening the door for a host of applications and improvements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning and Implementing a Basic RL Agent in PyTorch\n",
    "Reinforcement Learning (RL) is a branch of machine learning that trains software agents to make a sequence of decisions. The agent learns to perform actions by trial and error, by receiving rewards or penalties for these actions.\n",
    "\n",
    "Let's dive a bit deeper into the terminology of RL:\n",
    "* Agent: The learner and decision maker.\n",
    "* Environment: The world, through which the agent moves.\n",
    "* Action (A): What the agent can do. For example, moving up, down, left or right.\n",
    "* State (S): The current situation returned by the environment.\n",
    "* Reward (R): An immediate return sent back from the environment to evaluate the last action.\n",
    "* Policy (π): The strategy that the agent uses to determine the next action based on the current state. It's a map from state to action.\n",
    "* Value (V or Q): The future reward that an agent would receive by taking an action in a particular state.\n",
    "T\n",
    "here are many ways to implement RL agents, one of the most basic one is using Q-Learning.\n",
    "\n",
    "### Q-Learning\n",
    "Q-Learning is a values based algorithm in reinforcement learning. It uses a table (Q-table) where we calculate the maximum expected future rewards for action at each state. The goal is to maximize the value function Q. The Q function is defined as the immediate reward plus the maximum expected future reward.\n",
    "\n",
    "#### Q-Learning with PyTorch\n",
    "Let's now discuss how to implement a very basic Q-learning model with PyTorch. Suppose we're training an agent to play a simple game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q values: [0.23580271 0.10120855]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Simple model for Q learning\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        hidden_size = 8\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "state_size = 5\n",
    "action_size = 2\n",
    "qnetwork = QNetwork(state_size, action_size)\n",
    "\n",
    "# Example state\n",
    "state = np.array([1, 0, 0, 0, 0])\n",
    "\n",
    "# Convert state to tensor\n",
    "state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "# Compute Q values\n",
    "q_values = qnetwork(state_tensor)\n",
    "\n",
    "print(\"Q values:\", q_values.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define a simple neural network that takes a state (in our case, a 5-dimensional vector) and outputs a Q value for each possible action (in our case, 2 possible actions). The state represents the current condition of the environment, and the action is what our agent can do.\n",
    "\n",
    "This is a very simplistic example, and actual implementations will include more features such as an experience replay buffer to store and recall past experiences, an optimizer to update our QNetwork weights, a target Q network for more stable learning, and an epsilon-greedy strategy for exploration vs exploitation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
