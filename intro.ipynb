{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch\n",
    "### What is a Tensor? \n",
    "A tensor, in the context of PyTorch, is a multi-dimensional array used to represent data. It's similar to arrays in Python or ndarrays in NumPy, but with additional features that make it suitable for deep learning applications.\n",
    "\n",
    "### Installation of PyTorch\n",
    "Before we start coding, you need to install PyTorch. Assuming that you have Python installed, you can install PyTorch by running the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs not only PyTorch (`torch`), but also `torchvision` and `torchaudio`, which provides datasets and model architectures for computer vision and audio processing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with Tensors\n",
    "To start with PyTorch, we need to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a tensor with a specific data type using `torch.tensor()`. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "print(t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we've created a 1-dimensional tensor. You can also create a 2-dimensional tensor like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[1, 2], [3, 4]])\n",
    "print(t2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of a tensor (i.e., its dimensions) using the `.shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells you that `t2` is a 2x2 tensor (a matrix with 2 rows and 2 columns)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Operations\n",
    "Now let's dive into some mathematical operations that we can perform on tensors.\n",
    "\n",
    "##### Addition:\n",
    "\n",
    "You can add two tensors of the same shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = t1 + t2\n",
    "\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiplication:\n",
    "\n",
    "You can also multiply two tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = t1 * t2\n",
    "\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is element-wise multiplication, not matrix multiplication.\n",
    "\n",
    "##### Matrix Multiplication:\n",
    "\n",
    "For matrix multiplication, we can use `torch.matmul()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1, 2], [4, 3]])\n",
    "t2 = torch.tensor([[3, 4], [2, 1]])\n",
    "t3 = torch.matmul(t1, t2)\n",
    "\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, Slicing, Joining, and Mutation\n",
    "Let's explore how to manipulate tensors in PyTorch. The operations we will focus on are indexing, slicing, joining, and mutating tensors.\n",
    "\n",
    "#### Indexing and Slicing\n",
    "Like in Python and many other languages, indexing in PyTorch starts at 0. Let's create a 1-D tensor and access its elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# accessing the first element\n",
    "print(t1[0])\n",
    "\n",
    "# accessing the last element\n",
    "print(t1[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing works similarly to Python's list slicing. You can select a range of elements in a tensor using the syntax `[start:end]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1[1:3])  # prints elements at index 1 and 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and slicing for multi-dimensional tensors work similarly, just with additional dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# accessing a single element\n",
    "print(t2[1, 2])  # prints element at row 1, column 2\n",
    "\n",
    "# slicing\n",
    "print(t2[1:, :2])  # prints all rows starting from 1, and columns up to 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Tensors\n",
    "You can join multiple tensors into one. The most common way to do this is by using `torch.cat()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# concatenating along the 0th dimension\n",
    "t3 = torch.cat((t1, t2), dim=0)\n",
    "\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also concatenate 2-D tensors along either dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1, 2], [3, 4]])\n",
    "t2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "t3 = torch.cat((t1, t2), dim=0)  # concatenating along rows\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.cat((t1, t2), dim=1)  # concatenating along columns\n",
    "print(t4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation\n",
    "Finally, let's look at how to modify a tensor. You can change an element of a tensor by accessing it and assigning a new value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "\n",
    "t1[1] = 7  # changing the second element\n",
    "\n",
    "print(t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also modify a slice of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "t1[1:4] = torch.tensor([7, 8, 9])  # changing elements at index 1, 2, and 3\n",
    "\n",
    "print(t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that PyTorch tensors are mutable, meaning you can change their content without creating a new tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA support, Moving computations to GPU\n",
    "Deep learning models can be computationally intensive, and running them on a CPU can be slow. That's where Graphics Processing Units (GPUs) come in. GPUs are designed for performing large numbers of computations simultaneously, making them ideal for deep learning.\n",
    "CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by Nvidia for general computing on its own GPUs. PyTorch has built-in support for CUDA, which allows it to seamlessly perform computations on a GPU, if one is available.\n",
    "\n",
    "#### Checking for CUDA Availability\n",
    "Before we can move computations to a GPU, we first need to check whether CUDA is available. We can do this using `torch.cuda.is_available()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output `True` if CUDA is available and `False` otherwise.\n",
    "\n",
    "#### Moving Tensors to GPU\n",
    "You can create a tensor on a GPU directly, or move a tensor from CPU to GPU.\n",
    "\n",
    "To create a tensor on a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3], device='cuda:0')\n",
    "print(t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `cuda:0' refers to the first GPU. If you have multiple GPUs and you want to create a tensor on the second GPU, you would use 'cuda:1', and so on.\n",
    "\n",
    "To move a tensor from CPU to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([4, 5, 6])\n",
    "t2 = t2.to('cuda')\n",
    "\n",
    "print(t2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Computations on GPU\n",
    "Once a tensor is on a GPU, any operations performed on it are carried out on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3], device='cuda')\n",
    "t2 = torch.tensor([4, 5, 6], device='cuda')\n",
    "\n",
    "t3 = t1 + t2  # this computation is performed on the GPU\n",
    "\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to ensure that all tensors involved in an operation are on the same device. If they're not, you'll need to move them first. Note that while using a GPU can greatly speed up computations, it also comes with its own challenges, such as managing GPU memory. However, these are problems for a more advanced tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation with Autograd\n",
    "When training neural networks, we often need to compute the gradients of loss functions with respect to the model's parameters, which are then used to update the parameters. This can be done using a technique called backpropagation.\n",
    "Luckily, PyTorch provides a package called autograd for automatic differentiation, which simplifies the computation of backward passes. This is an essential tool for neural networks, and PyTorch makes it quite easy to use.\n",
    "\n",
    "#### Tensors and Gradients\n",
    "In PyTorch, you can tell an autograd-compatible tensor to remember its operations for backpropagation using `.requires_grad_(True)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "print(t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor `t1` now has the ability to keep track of every operation involving it. Now, let's perform some operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t1 * 2\n",
    "t3 = t2.mean()\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `t3` is created as a result of operations involving `t1`, so it has a `grad_fn`.\n",
    "#### Backward Propagation\n",
    "To perform backpropagation, we call `.backward()` on the tensor. This computes the gradient of `t3` with respect to `t1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling `.backward()`, the gradients are stored in the `.grad` attribute of the original tensor (`t1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient stored in `t1.grad` is the partial derivative of `t3` with respect to `t1`. It's important to note that if `t1`'s shape doesn't match with `t3`, PyTorch won't be able to directly compute the gradients. That's why when we are dealing with scalar loss in a neural network, we don't have to provide any arguments to `backward()`. Also, PyTorch accumulates the gradient in the `.grad` attribute, which means that it doesn't overwrite the previous values but instead adds the newly computed gradient. To clear the gradients for a new operation, you need to call `.grad.zero_()`.\n",
    "\n",
    "### PyTorch Libraries: Torchtext, torchvision, torchaudio\n",
    "PyTorch is not just a deep learning library; it also has several companion libraries that provide additional functionality, such as handling specific types of data and performing various common operations. Today, we'll discuss three of these libraries: `torchtext`, `torchvision`, and `torchaudio`.\n",
    "\n",
    "#### Torchtext\n",
    "\n",
    "`torchtext` is a library for handling text data. It provides tools for creating datasets, handling tokenization, and managing vocabularies. It also includes several common datasets and pre-trained embeddings (like Word2Vec or GloVe). Here is a simple example of using `torchtext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data import TabularDataset\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "# Define fields\n",
    "def tokenize_text(text):\n",
    "    return [token.lower() for token in tokenizer(text)]\n",
    "\n",
    "# Define dataset\n",
    "dataset = TabularDataset(path=\"mydata.csv\", format='csv', fields=[(\"text\", tokenize_text), (\"label\", LABEL)])\n",
    "\n",
    "# Build vocab\n",
    "counter = Counter()\n",
    "for (text, label) in dataset:\n",
    "    counter.update(tokenize_text(text))\n",
    "vocab = Vocab(counter, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torchvision\n",
    "\n",
    "`torchvision` is a library for handling image data. It includes tools for transforming images, common image datasets (like CIFAR10, MNIST), and pre-trained models (like ResNet, VGG). Here is a simple example of using `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load a dataset\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# torchvision also provides pre-trained models\n",
    "from torchvision import models\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torchaudio\n",
    "\n",
    "`torchaudio` is a library for handling audio data. It provides tools for loading and saving audio data, transformations (like spectrograms, mel-scaling), and some common audio datasets. Here is a simple example of using `torchaudio`:\n",
    "\n",
    "##### Audio I/O backend\n",
    "`torchaudio` needs an audio I/O backend called SoundFile to read and write audio files. Install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SoundFile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some common stuff that `torchaudio` is used for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Load an audio file\n",
    "waveform, sample_rate = torchaudio.load('audio.wav') # Replace with a WAV file on your local drive\n",
    "\n",
    "# Apply a transform\n",
    "transform = torchaudio.transforms.MFCC(sample_rate=sample_rate)\n",
    "mfcc = transform(waveform)\n",
    "\n",
    "# torchaudio also includes some common datasets\n",
    "yesno_data = torchaudio.datasets.YESNO('.', download=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
