{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Basics in PyTorch\n",
    "### Building a Basic Feedforward Neural Network\n",
    "A feedforward neural network, also known as a multi-layer perceptron (MLP), is a basic type of neural network where information moves in only one direction—it is fed forward from the input layer, through any number of hidden layers, to the output layer. There are no cycles or loops in the network.\n",
    "\n",
    "#### Building a Feedforward Neural Network with PyTorch\n",
    "With PyTorch, we can easily build a feedforward neural network by defining a class that inherits from torch.nn.Module. Let's start with a simple network that has one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "First, we need to import the torch module and the torch.nn module, which contains the building blocks needed to build neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 10)  # 10 input neurons, 10 output neurons\n",
    "        self.fc2 = nn.Linear(10, 1)   # 1 output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation function on first layer\n",
    "        x = self.fc2(x)              # Output layer\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of the code:\n",
    "\n",
    "* We create a class Net that inherits from nn.Module.\n",
    "* In the constructor (_ _ init _ _), we define two fully connected layers (fc1 and fc2). nn.Linear is used to apply a linear transformation to the incoming data: y = x*A^T + b.\n",
    "* forward(x) is a method that implements the forward propagation of the input x. torch.relu is the Rectified Linear Unit (ReLU) activation function that is applied elementwise.\n",
    "\n",
    "We can now create an instance of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the network architecture, the next step would be to define a loss function and an optimizer, pass the input data through the network, calculate the loss, backpropagate the error, and update the weights. These steps are typically performed in a training loop, which we will discuss in a bit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Feedforward Neural Network\n",
    "Training a neural network involves iterating over a dataset of input samples and adjusting the weights of the network to minimize a loss function.\n",
    "\n",
    "Let's continue with the feedforward neural network we built in the previous lesson. We'll use a simple regression problem where the task is to predict a single target value from 10 input features. We'll create an artificial dataset using torch.randn() for this purpose.\n",
    "\n",
    "#### Preparing the Data\n",
    "Let's generate a simple synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = torch.randn(100, 10)  # 100 samples, 10 features each\n",
    "target_values = torch.randn(100, 1)    # 100 target values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates two tensors: input_features containing 100 samples each with 10 features, and target_values containing 100 corresponding target values. The values are randomly generated.\n",
    "\n",
    "#### Defining a Loss Function and an Optimizer\n",
    "We'll use Mean Squared Error (MSE) as our loss function—it's a common choice for regression problems. For optimization, we'll use Stochastic Gradient Descent (SGD). PyTorch provides these as torch.nn.MSELoss and torch.optim.SGD respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)  # net is our network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer adjusts the weights of the network based on the gradients computed during backpropagation.\n",
    "#### Training the Network\n",
    "Training involves a loop over epochs (iterations over the whole dataset). In each epoch, we:\n",
    "1. Run the forward pass to get the outputs from the input features,\n",
    "2. Calculate the loss between the outputs and the target values,\n",
    "3. Backpropagate the error through the network,\n",
    "4. Use the optimizer to adjust the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):  # Number of times to loop over the dataset\n",
    "    # Forward pass\n",
    "    outputs = net(input_features)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(outputs, target_values)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Perform backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code, you'll see the loss decreasing over epochs, which means our model is learning to fit the input features to the target values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Feedforward Neural Network\n",
    "After training a neural network, it's crucial to evaluate its performance on a separate test dataset that the model has never seen during training. This helps to determine if the model is generalizing well or just memorizing the training data (overfitting).\n",
    "We will continue from the feedforward neural network model we trained in the previous lesson.\n",
    "\n",
    "#### Preparing the Test Data\n",
    "Similar to the training data, we'll create synthetic test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.randn(50, 10)  # 50 samples, 10 features each\n",
    "test_targets = torch.randn(50, 1)    # 50 target values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model\n",
    "Now, let's evaluate our model on this test data. While doing so, we'll switch off PyTorch's autograd engine using torch.no_grad(). This reduces memory usage and speeds up computations but is unable to perform backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Switch off gradients\n",
    "    test_outputs = net(test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Loss\n",
    "To calculate the loss between our model's predictions and the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = loss_fn(test_outputs, test_targets)\n",
    "print(f'Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print the test loss value. In a perfect model, this would be 0, but in practice it's always higher. Ideally, this test loss should be close to the training loss. If the test loss is much higher, it might be a sign that the model is overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation and Computational Graphs\n",
    "Backpropagation is a critical process in training deep learning models. It is a method used to calculate the gradient of the loss function with respect to each weight in the model, which is then used to update the weights. Backpropagation is made possible in PyTorch through the construction of computational graphs during the forward pass. In this lesson, we will discuss computational graphs and backpropagation in more detail.\n",
    "\n",
    "#### What is a Computational Graph?\n",
    "A computational graph is a directed graph where the nodes correspond to operations or variables. Variables can be input tensors, parameters (such as weights and biases), or intermediate values. Operations can be any mathematical operations, such as addition and multiplication, or more complex ones like activation functions.\n",
    "\n",
    "When a forward pass is made in a neural network, a computational graph is dynamically created in PyTorch. This graph is used to compute the gradients during the backward pass, which is a fundamental step in optimizing the network.\n",
    "\n",
    "#### Backpropagation and Computational Graphs\n",
    "Let's consider a simple example to understand how PyTorch constructs a computational graph and how backpropagation is performed. Let's take two tensors a and b, which are parameters of the model and require gradient computation, and a simple equation f = (a * b) + b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize tensors\n",
    "a = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "# Compute outputs\n",
    "f = (a * b) + b\n",
    "\n",
    "# Compute gradients\n",
    "f.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(a.grad)  # df/da\n",
    "print(b.grad)  # df/db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a computational graph is created in the following way:\n",
    "\n",
    "Nodes are created for the tensors a and b.\n",
    "When f = (a * b) + b is computed, nodes are created for the operations * and +, and for the output f. Edges are created to connect these operations.\n",
    "During the backward pass (f.backward()), the gradients of the output with respect to each tensor that has requires_grad=True are computed using this computational graph.\n",
    "The returned gradients are the derivatives of f with respect to a and b and are stored in the .grad property of the respective tensors.\n",
    "\n",
    "In more complex models like deep neural networks, the computational graph can be far more intricate, but the idea remains the same. During backpropagation, the gradients of the loss with respect to the weights are computed and used to update the weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Networks using nn.Module\n",
    "We've already touched on the usage of nn.Module in PyTorch when building our simple feedforward network, but it's worth diving deeper into how it works and why it's such a fundamental part of structuring models in PyTorch.\n",
    "\n",
    "nn.Module is a base class for all neural network modules in PyTorch. Your models should also subclass this class, which allows you to utilize powerful PyTorch functionalities. Modules can contain other Modules, allowing to nest them in a tree structure.\n",
    "\n",
    "#### Building a Network with nn.Module\n",
    "Let's expand our basic feedforward network into a multi-layer one using nn.Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Input layer to hidden layer\n",
    "        self.hidden = nn.Linear(10, 20) \n",
    "\n",
    "        # Hidden layer to output layer\n",
    "        self.output = nn.Linear(20, 1)  \n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()             \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)  # Pass data through hidden layer\n",
    "        x = self.relu(x)    # Apply activation function\n",
    "        x = self.output(x)  # Pass data through output layer\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Net class defined above, we first define the layers of our network in the constructor __init__(). We have an input to hidden layer connection (self.hidden), an activation function (self.relu), and a hidden to output layer connection (self.output).\n",
    "\n",
    "Then, in the forward function, we define the path that the input tensor will take through the network.\n",
    "\n",
    "#### Why nn.Module?\n",
    "* Parameter management: Once layers are defined as members of nn.Module (in our example, self.hidden and self.output), all parameters of these layers are registered in the module. You can access all parameters of your network using the parameters() or named_parameters() methods.\n",
    "* Device management: You can recursively move all parameters of the network to a device (CPU or GPU) by calling .to(device) on the network. This saves you the trouble of moving each tensor individually.\n",
    "* Easy to use with optimizers: You can pass the parameters of a network (retrieved through net.parameters()) to an optimizer, and it will take care of the parameter updates for you.\n",
    "* Helper methods and functionalities: There are many other functionalities provided by nn.Module such as the ability to save and load the model, ability to set the network in training or evaluation mode (net.train(), net.eval())."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging PyTorch models, Using torchviz to visualize networks\n",
    "Debugging PyTorch models and visualizing the computational graph of your model is an essential part of understanding and developing deep learning models. PyTorch does not come with a built-in tool for this, but a popular third-party library, torchviz, can be used to create a visualization of your model.\n",
    "\n",
    "#### Debugging PyTorch Models\n",
    "Debugging PyTorch models can be done in the same way as debugging any Python program, using Python's built-in debugging tools. The most popular is pdb, the Python debugger. You can insert breakpoints into your code using pdb.set_trace(), which allows you to step through your code and inspect variables.\n",
    "\n",
    "For example, during the forward pass of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pdb.set_trace()  # Set breakpoint here\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the forward method is called, the program will pause at pdb.set_trace(). You can then print the values of variables, step through the code, and more. To continue execution, type c and hit enter.\n",
    "\n",
    "#### Visualizing Networks with torchviz\n",
    "torchviz is a PyTorch specific package that creates directed graphs of PyTorch execution traces and computational graphs. It's handy to understand the operations and tensor transformations that take place during the forward and backward pass.\n",
    "\n",
    "To use torchviz, you first need to install it, and also be sure to download the installer from https://graphviz.org/download/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use it to visualize the computation graph of a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('W0', nn.Linear(8, 16))\n",
    "model.add_module('tanh', nn.Tanh())\n",
    "model.add_module('W1', nn.Linear(16, 1))\n",
    "\n",
    "x = torch.randn(1,8)\n",
    "\n",
    "# Forward pass\n",
    "y = model(x)\n",
    "\n",
    "# Create a visualization of the forward computation graph\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"rnn_torchviz\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
